---
title: "Autonomous Resistor Sorter - Blog #2"
date: 2022-09-24


---

## Fabricating the Physical Subsystems

For the physical components of the resistor sorter, we are in the build stage. We have addressed our previous issue with the CNC machine being down with an adjusted design and it is now being machined.

![Machining aluminum shaft](/Autonomous-Resistor-Sorter/assets/images/post_09_24_22/aluminum_shaft.png "Machining aluminum shaft.")

While our critical milestone, the main aluminum shaft, is still being constructed, we have built and acquired the other parts for the delivery subsystem. We have built the delivery subsystem’s main body as well as the main support for the storage subsystem. Additionally, we have ordered our servos and standard parts for construction.

![Constructing subsystems](/Autonomous-Resistor-Sorter/assets/images/post_09_24_22/construction_1.png "Constructing the physical subsystems.") ![Constructing subsystems](/Autonomous-Resistor-Sorter/assets/images/post_09_24_22/construction_2.png "Constructing the physical subsystems.") ![Servo motors](/Autonomous-Resistor-Sorter/assets/images/post_09_24_22/construction_3.png "Servo motors.")

Now the delivery and storage subsystems only need the 3D printed parts and the main shaft to complete construction. This will be our focus for the next week to ensure our assembly deadline is met. We were challenged over the past two weeks to ensure that completing the fabrication of the main aluminum shaft, a critical milestone, will be met by our stated goal of September 30th. I am now more confident that this will happen.


## Progress on the Separator Subsystem

During the last two weeks, for the resistor separation subsection, the manufacturing of the separation cylinder was the primary focus. The final parts of the prototype have been printed and assembled. All parts align and several iterations of a random number of resistors have passed through without issue.

The main focus for the next two weeks will be the transporting the resistors between the separation cylinder and the belt. We have scheduled a meeting with a practicing engineer with experience in industrial automation to discuss the optimal approach to fabricate the ramp which will carry the resistors from the separation cylinder to the belt. However, to allow earlier testing of the separator subsystem, we will instead use the straight track from a Hot Wheels set, shown below. When the fabricated ramp arrives, a full prototype of the separation cylinder will be built, and further tests will be conducted.

![Substitute track](/Autonomous-Resistor-Sorter/assets/images/post_09_24_22/hotwheels.png "Temporary track used for separator system tests.")
Temporary track used for separator system tests.


## Machine Learning Vision System Progress

With the solution to identifying colors found, the past two weeks were spent optimizing the model to increase its inference speed.

First, let's discuss the different model inference approaches we tested. The default method is to invoke the interpreter to find objects in the image that pass a certain accuracy threshold. This method is fast, but is not accurate for small objects such as resistors that only take up a partial amount of the image. The second method we used is tiling. Tiling is used for small object detection and it splits the image into sub tiles which are individually passed to the model and reconstructed into one big image later. This increases the accuracy for finding small objects by “zooming” into the objects but comes at a cost of inference speed.

We found that using the default approach of finding objects in an image resulted in an accuracy that was too low. Furthermore, we found tiling did increase our accuracy, however, the increase of the inference time was prohibitively large for our application. The problem with tiling for a single resistor is that since there is only one resistor in the frame, most of the tiles sent to the model are empty. If we had multiple resistors in the frame, then there might be a benefit of using tiling as more tiles would also include resistors. To address this issue, we decided to make a second model specifically dedicated to detecting the one resistor in the image. The model would find the resistor and return an image crop of only the resistor, thus “zooming” into the image and returning the information we care about without any useless background. The second model in the sequence would then detect the location of the resistor bands within the image crop. For this, we don’t need to implement tiling because the bands are not small objects within the frame anymore due to the crop. Finally, the band crops would be sent to our third and final model to determine the color of the bands, as discussed in the last blog post. A pipeline of this approach can be seen below.

![Raw resistor image](/Autonomous-Resistor-Sorter/assets/images/post_09_24_22/resistor_1.png "Raw resistor image.")
Raw image fed to the first model.

![Detecting resistor location](/Autonomous-Resistor-Sorter/assets/images/post_09_24_22/resistor_2.png "Detecting resistor location.")
First model detects the resistor.

![Detecting band location](/Autonomous-Resistor-Sorter/assets/images/post_09_24_22/resistor_3.png "Detecting the location of the resistor bands.")
The band detecting model only looks at the cropped image and returns the locations of the bands, which are then passed to the color detecting model.

Below are the summary statistics from the resistor detecting model as well as the band detecting model.

![Resistor detecting model results](/Autonomous-Resistor-Sorter/assets/images/post_09_24_22/results_1.png "Results from the resistor detecting model.")
Results from the resistor detecting model.

![Band detecting model results](/Autonomous-Resistor-Sorter/assets/images/post_09_24_22/results_2.png "Results from the band detecting model.")
Results from the band detecting model.

The most important metric shown here is the "AP" or Average Precision. These values are acceptable for our model and will increase with further data collected. To test these models, we only created a small dataset of 90 images as a proof of concept. Now that we know this approach is feasible, we will now proceed to build our final dataset consiststing of a larger number of images.

### Risks with the Proposed Approach

The biggest risk with this approach comes from the hardware constraints we face by using a small edge device. The Google Coral contains a limited amount of cache memory. This cache is meant to hold the model parameters from one model. When we actively use two models, the model parameters from the first model are stored in cache, then kicked out when the second model is invoked. At this point, the second model parameters are moved to cache and this back and forth repeatedly occurs as both model parameters are exchanged in the cache. One approach to fix this problem is to co-compile the models. When the models are co-compiled, the device stores the first model’s parameters in cache and fills the remaining cache with parameters from the second model. Then, when the second model is invoked, the first model’s parameters do not get kicked out of cache. Rather, the second model’s remaining parameters are fetched directly from the device's read-only memory instead. After additional testing, if we find that this hardware constraint is a problem, we will puchase a second Google Coral device and have each model run on each device separately.

### Future Steps

The only additional step to complete the implementation of the machine learning visions system, beyond what was previously discussed, is to further build the dataset for the models. We have already invested time upfront towards developing a program that will semi-automate the image collection process, allowing us to quickly and easily develop a large production dataset. Now, we simply need to collect the images and label them. This should take no longer than one day to finish. After that, it will be time to deploy the final, trained model onto the Google Coral edge device.